# NoisyNeuroSIM

This repository was created to depict the work on my master thesis on the topic "Noise-aware Quantization Flow for Deep Learning Acceleration with Analog In-Memory-Computing", supervised by Rebecca Pelke and José Cubero, at the Institute for Communication Technologies and Embedded Systems, RWTH Aachen University.


# Table of Content

1. How to get a HPC Account
2. How to set up NeuroSIM on HPC cluster
3. How to run NeuroSIM on HPC cluster
4. Useful command

-------------------------------------------------------------------------

# 1. How to get a HPC Account


# 2. How to set up NeuroSIM on HPC cluster


# 3. How to run NeuroSIM on HPC cluster


# 4. Useful commands




# References
[1] Shimeng Yu et. all, Compute-in-Memory Chips for Deep Learning: Recent Trends and Prospects. IEEE MCAS. August 2021. https://doi.org/10.1109/MCAS.2021.3092533   <br />
[2] X. Peng,et. all. DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for Training, § IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, doi: 10.1109/TCAD.2020.3043731, 2020.  <br />
[3] S. K. Gonugondla, Fundamental Limits on Energy-Delay-Accuracy of In-Memory Architectures in Inference Applications, in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 10, pp. 3188-3201, Oct. 2022, doi: 10.1109/TCAD.2021.3124757.  <br />
[4] Nathan Laubeuf et all. Dynamic Quantization Range Control for Analog-in-Memory Neural Networks Acceleration. ACM Trans. Des. Autom. Electron. Syst. 27, 5, Article 46 (September 2022), 21 pages. https://doi.org/10.1145/3498328  <br />
[5] Fundamental Limits on the Precision of In-memory Architectures https://ieeexplore.ieee.org/document/9256802  <br />
[6] Joshi, V., Le Gallo, M., Haefeli, S. et al. Accurate deep neural network inference using computational phase-change memory. Nat Commun 11, 2473 (2020). https://doi.org/10.1038/s41467-020-16108-9  <br />
[7] Mackin, C., Rasch, M.J., Chen, A. et al. Optimised weight programming for analogue memory-based deep neural networks. Nat Commun 13, 3765 (2022). https://doi.org/10.1038/s41467-022-31405-1   <br />
[8] C. -Y. Chang, K. -C. Chou, Y. -C. Chuang and A. -Y. Wu, "E-UPQ: Energy-Aware Unified Pruning-Quantization Framework for CIM Architecture," in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 13, no. 1, pp. 21-32, March 2023, doi: 10.1109/JETCAS.2023.3242761. <br /> 

# Background

Analog In-Memory-Computing (AIMC) has emerged as an attractive alternative to conventional von Neumann (digital) architectures for addressing the energy and latency cost of memory accesses in data-centric machine learning workloads. Analog IMCs embed low-energy analogue computations in the memory array to execute machine learning computations such as matrix-vector multiply (MVM) and dot products (DPs). However, converting data between digital and analogue domains has a cost that must be carefully studied.



